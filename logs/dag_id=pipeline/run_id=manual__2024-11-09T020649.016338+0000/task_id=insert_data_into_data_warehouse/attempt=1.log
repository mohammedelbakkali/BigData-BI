[2024-11-09T02:06:52.395+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-11-09T02:06:52.419+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: pipeline.insert_data_into_data_warehouse manual__2024-11-09T02:06:49.016338+00:00 [queued]>
[2024-11-09T02:06:52.427+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: pipeline.insert_data_into_data_warehouse manual__2024-11-09T02:06:49.016338+00:00 [queued]>
[2024-11-09T02:06:52.428+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-11-09T02:06:52.439+0000] {taskinstance.py:2330} INFO - Executing <Task(_PythonDecoratedOperator): insert_data_into_data_warehouse> on 2024-11-09 02:06:49.016338+00:00
[2024-11-09T02:06:52.443+0000] {standard_task_runner.py:64} INFO - Started process 196 to run task
[2024-11-09T02:06:52.447+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'pipeline', 'insert_data_into_data_warehouse', 'manual__2024-11-09T02:06:49.016338+00:00', '--job-id', '174', '--raw', '--subdir', 'DAGS_FOLDER/pipline.py', '--cfg-path', '/tmp/tmpbsj8dc1g']
[2024-11-09T02:06:52.450+0000] {standard_task_runner.py:91} INFO - Job 174: Subtask insert_data_into_data_warehouse
[2024-11-09T02:06:52.494+0000] {task_command.py:426} INFO - Running <TaskInstance: pipeline.insert_data_into_data_warehouse manual__2024-11-09T02:06:49.016338+00:00 [running]> on host 1ab9a13efafa
[2024-11-09T02:06:52.608+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='pipeline' AIRFLOW_CTX_TASK_ID='insert_data_into_data_warehouse' AIRFLOW_CTX_EXECUTION_DATE='2024-11-09T02:06:49.016338+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-11-09T02:06:49.016338+00:00'
[2024-11-09T02:06:52.609+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-11-09T02:06:52.609+0000] {logging_mixin.py:188} INFO - Les données sont une liste. Accès au premier élément.
[2024-11-09T02:06:52.610+0000] {logging_mixin.py:188} INFO - Données traitées : {'_id': '672e9db28c1b2fbf276eef12', 'Title': 'Assessing Trustworthy AI in Times of COVID-19: Deep Learning for Predicting a Multiregional Score Conveying the Degree of Lung Compromise in COVID-19 Patients', 'DOI': '10.1109/TTS.2022.3195114', 'Authors': ['Himanshi Allahabadi', 'Julia Amann', 'Isabelle Balot', 'Andrea Beretta', 'Charles Binkley', 'Jonas Bozenhard'], 'Publication Date': 'Date of Publication: 29 July 2022', 'ISSN': {'Print ISSN': 'Non disponible', 'Electronic ISSN': '2637-6415', 'CD': 'Non disponible'}, 'Link': 'https://ieeexplore.ieee.org/document/9845195/', 'Quartils': 'Journal pas indexé Scopus', 'journal_main': 'Published in: IEEE Transactions on Technology and Society ( Volume: 3, Issue: 4, December 2022)', 'abstract': 'This article’s main contributions are twofold: 1) to demonstrate how to apply the general European Union’s High-Level Expert Group’s (EU HLEG) guidelines for trustworthy AI in practice for the domain of healthcare and 2) to investigate the research question of what does “trustworthy AI” mean at the time of the COVID-19 pandemic. To this end, we present the results of a post-hoc self-assessment to evaluate the trustworthiness of an AI system for predicting a multiregional score conveying the degree of lung compromise in COVID-19 patients, developed and verified by an interdisciplinary team with members from academia, public hospitals, and industry in time of pandemic. The AI system aims to help radiologists to estimate and communicate the severity of damage in a patient’s lung from Chest X-rays. It has been experimentally deployed in the radiology department of the ASST Spedali Civili clinic in Brescia, Italy, since December 2020 during pandemic time. The methodology we have applied for our post-hoc assessment, called Z-Inspection®, uses sociotechnical scenarios to identify ethical, technical, and domain-specific issues in the use of the AI system in the context of the pandemic.'}
[2024-11-09T02:06:52.617+0000] {base.py:84} INFO - Using connection ID 'mysql_default' for task execution.
[2024-11-09T02:06:52.645+0000] {logging_mixin.py:188} INFO - Erreur lors de l'insertion des données : string indices must be integers, not 'str'
[2024-11-09T02:06:52.648+0000] {logging_mixin.py:188} INFO - Connexion MySQL fermée.
[2024-11-09T02:06:52.648+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-11-09T02:06:52.649+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-11-09T02:06:52.657+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=pipeline, task_id=insert_data_into_data_warehouse, run_id=manual__2024-11-09T02:06:49.016338+00:00, execution_date=20241109T020649, start_date=20241109T020652, end_date=20241109T020652
[2024-11-09T02:06:52.699+0000] {local_task_job_runner.py:243} INFO - Task exited with return code 0
[2024-11-09T02:06:52.718+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-11-09T02:06:52.722+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
